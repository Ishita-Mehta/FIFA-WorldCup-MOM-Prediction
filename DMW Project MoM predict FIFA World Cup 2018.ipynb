{"cells":[{"metadata":{"_uuid":"4634dc8cb88e3b4765b8f7dcaf0620aa1179c57c"},"cell_type":"markdown","source":"# Exploratory Data Analysis: FIFA World Cup 2018\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.myuganda.co.ug/wp-content/uploads/2018/06/2018-Fifa-Worldcup-in-Russia.jpg)"},{"metadata":{"_uuid":"076d45b877ff5bbdd02a9c4d2f1dabb4feb01d84"},"cell_type":"markdown","source":"Importing the required packages."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n\nimport os\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8f8ea1579bef2f387c3f8b932568963606f406b"},"cell_type":"markdown","source":"### A Look At The Dataset\n    "},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b3237dead14c52dffcc86524927c72a9dc0caba5"},"cell_type":"code","source":"data_file = pd.read_csv('../input/FIFA 2018 Statistics.csv')\n\n# Creating a new Feature 'Result' to depict whether the match was a {'Win':1, 'Draw':0, 'Loss':-1} \ndata_file['Result']=np.nan\nfor i in range(0,128,2):\n    a_goals=data_file.iloc[i]['Goal Scored']\n    b_goals=data_file.iloc[i+1]['Goal Scored']\n    if a_goals>b_goals:\n        data_file.iloc[i,-1]=1\n        data_file.iloc[i+1,-1]=-1\n    elif b_goals>a_goals:\n        data_file.iloc[i,-1]=-1\n        data_file.iloc[i+1,-1]=1\n    else:\n        data_file.iloc[i,-1]=0\n        data_file.iloc[i+1,-1]=0\n        \n#Changing the categorical feature 'Man of the Match' :'Yes', 'No' to 1, 0\ndata_file.rename(columns={'Man of the Match':'MoM'},inplace=True)\ndata_file.MoM = data_file.MoM.replace(['Yes','No'],[1,0])\n\ndata_file.head()  #displays the first 5 rows of the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_file.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_df = pd.DataFrame()\nmissing_df['Feature'] = data_file.isnull().sum().sort_values(ascending = False).index\nmissing_df['Count of Missing Values'] = data_file.isnull().sum().sort_values(ascending = False).values\nmissing_df['Percentage of Missing Values'] = missing_df['Count of Missing Values']*100 / len(data_file)\nmissing_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_file['Own goals'].fillna(0, inplace = True)\ndata_file['Own goal Time'].fillna(0, inplace = True)\ndata_file['1st Goal'].fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f62c4f06d92ffdbd6b142d5a6a39bf5ebd8feb2"},"cell_type":"markdown","source":"#### **Understanding the Features**\n\n1. **Date**: Match Date.\n2. **Team**: Playing Team.\n3. **Opponent**: Opponent Team.\n4. **Goal Scored**: Number of goals scored by playing team.\n5. **Ball Possession %**: Amount of time the ball was in control of the playing team.\n6. **Attempts**: Number of attempts to score a goal.\n7. **On-Target**: Number of shots that were on-target.\n8. **Off-Target**: Number of shots that were off-target.\n9. **Blocked**: Number of opponent team's shots blocked.\n10. **Corners**: Number of corner shots.\n11. **Offsides**: Number of offside plays.\n12. **Free Kicks**: Number of free kicks used.\n13. **Saves**: Number of saves by the goal keeper.\n14. **Pass Accuracy %**: Percentage of passes that reached the same team player as aimed.\n15. **Passes**: Total number of passes by the team.\n16. **Distance Covered(Kms)**: Total distance covered by the team members in this game.\n17. **Fouls Committed**: Number of fouls committed by the team members.\n18. **Yellow Card**: Number of Yellow warning received.\n19. **Yellow & Red**: Number of Yellow & Red warning received.\n20. **Red**: Number of Red cards received.\n21. **Man of the Match**: Did this team member win Man of the Match?\n22. **1st Goal**: When did the team score the 1st goal?\n23. **Round**: Stage of the match\n24. **PSO**: Was there a penalty shootout (PSO) in this match?\n25. **Goals in PSO**: Number of goals scored in the Penalty shootout.\n26. **Own goals**: Number of own goals.\n27. **Own goal Time**: When did the team score own goal?"},{"metadata":{"_uuid":"d8fc176f5a56e2684aa7b3e7b0826f071f6d70b5"},"cell_type":"markdown","source":"####  Detecting Any Outliers "},{"metadata":{"trusted":true,"_uuid":"057f05c0abc48edb39db9272a51655ebda959ba7"},"cell_type":"code","source":"#Detecting Outliers\nvar1 = ['Goal Scored', 'On-Target', 'Corners', 'Attempts', 'Free Kicks', 'Yellow Card', 'Red', 'Fouls Committed']\ndummy_data = data_file[var1]\nplt.figure(figsize=(30,20))\nsns.boxplot(data = dummy_data)\nplt.title('Detecting Outliers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_outliers(x):\n    q1 = x.quantile(.25)\n    q3 = x.quantile(.75)\n    iqr = q3 - q1\n    floor = q1 - 1.5*iqr\n    ceiling = q3 + 1.5*iqr\n    outlier_indices = list(x.index[(x < floor) | (x > ceiling)])\n    outlier_values = list(x[outlier_indices])\n    #data = data[~((data < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 * IQR))).any(axis=1)]\n    #boston_df_out.shape\n    return outlier_indices, outlier_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = ['Goal Scored','Ball Possession %', 'Attempts', 'On-Target', 'Off-Target',\n           'Blocked', 'Corners', 'Offsides', 'Free Kicks',\n           'Saves', 'Pass Accuracy %', 'Passes', 'Distance Covered (Kms)', 'Fouls Committed',\n           'Yellow Card', 'Yellow & Red', 'Red', 'MoM', '1st Goal', 'Goals in PSO',\n           'Own goals', 'Own goal Time', 'Result']\n\nmaster_indices = []\nfor i in num_cols:\n    indices, vals = find_outliers(data_file[i])\n    master_indices.append(indices)\n    print(\"Outliers for \", i)\n    print(\"Total = \", len(vals))\n    #data_file.drop(indices, inplace = True)\n    print(np.sort(vals))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48b37e34d0ec2ebc6685edac76389d740b987fda"},"cell_type":"markdown","source":"The outliers haven't been removed as the dataset is small."},{"metadata":{"_uuid":"98f2d446e6d249108f023bdd37f837cb1d6a984a"},"cell_type":"markdown","source":"#### **Summary of the dataset**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"60e2d4ccd2f5c682567afb91bb830dcfbc92f977"},"cell_type":"code","source":"data_file.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cfcfea02394a83aa715fbc7432867c3c3d51274"},"cell_type":"markdown","source":"### **Insights Into The Data**"},{"metadata":{"_uuid":"614a4d1b40d736280faf362bc0d42f2ad629a96c"},"cell_type":"markdown","source":"#### **Univariate Analysis**"},{"metadata":{"trusted":true,"_uuid":"8a7c880f756ab8316d4c10e4b8b06ce0088b906a"},"cell_type":"code","source":"plt.figure(figsize = (30,10))\nGoals_sorted_sum = data_file.groupby('Team')['Goal Scored'].sum().reset_index().sort_values(by=('Goal Scored'), ascending=True)\n\nsns.set(font_scale=3.5)\nplot1 = sns.barplot(x='Team', y='Goal Scored', data=Goals_sorted_sum)\n\nplot1.set_xticklabels(Goals_sorted_sum['Team'], rotation=90, ha=\"center\")\nplot1.set(xlabel='Teams',ylabel='Goals Scored')\nplot1.set_title('Goals Scored by Teams')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15697fb643cb189cd9cbb76cb9d2de16e3c7d2ce"},"cell_type":"markdown","source":"The maximum number of goals scored were by Belgium. "},{"metadata":{"_uuid":"801b1e4ca14b233a93e71522a1077b0d54f6fd75"},"cell_type":"markdown","source":"#### Features Distribution"},{"metadata":{"trusted":true,"_uuid":"5687b7f2c0e07e4def0ee05405d09383cc644903"},"cell_type":"code","source":"def plot_cols(arr):\n    plt.figure(figsize=(40,40))\n    for i in range(len(arr)):\n        plt.subplot(4,3,i+1)\n        plt.title(arr[i])\n        data_file[data_file['Result']==1][arr[i]].plot(kind='kde',legend=True,label='wins')\n        data_file[data_file['Result']==-1][arr[i]].plot(kind='kde',legend=True,label='losses')\n        data_file[data_file['Result']==0][arr[i]].plot(kind='kde',legend=True,label='draws')\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72d653b92612754ebc20c608ceeb0f6370a78270"},"cell_type":"code","source":"plot_cols(['Attempts', 'Goal Scored', 'Off-Target', 'On-Target', 'Fouls Committed', 'Offsides', 'Ball Possession %', 'Passes','Corners','Pass Accuracy %','Distance Covered (Kms)','Saves'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b8d7a364355ce2b9d2fdb60e615af8f1738bfab"},"cell_type":"markdown","source":"From the above distribution it is seen that Teams which have won have more Attempts, Off-Targets, Pass Accuracy, Saves. "},{"metadata":{"_uuid":"ecaaaec0be36dcb2b1532786d4215d1ba5fed9a4"},"cell_type":"markdown","source":"## Bivariate Analysis\n### Correllation Plot"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8329fa0f30469c2778d6b457d508e1a786686339"},"cell_type":"code","source":"sub = ['Attempts', 'Goal Scored', 'Off-Target', 'On-Target', 'Fouls Committed', 'Offsides', 'Ball Possession %', 'Passes','Corners','MoM','Pass Accuracy %','Distance Covered (Kms)','Saves']\ndata = data_file[sub]\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.set(font_scale=3.5)\nplt.subplots(figsize = (35,35))\nsns.heatmap(data.corr(), \n            annot=True,\n            cmap = cmap,\n            linewidths=0.1, \n            linecolor='white',\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74556b95dfcbc0d278fd1f5b1decb0d8acb5098a"},"cell_type":"markdown","source":"From the Heat Map above, it can be inferred that: \n\nThere is a correlation between **Ball Possession and Passes**. This is true as the more number of passes made by a team means that the ball is in their possession.\n\nThere is a correlation between **Number of Attempts and number of On-Target and Off-Target shots**. This correlation seems logical, the more number of attempts at scoring a goal will lead to more number of shots.\n\n**Ball Possession and Pass Accuracy %** also seems to have a correlation as if the team has more possession of the ball they seem to have passed the ball more. More passes will obviously lead to higher Pass Accuracy %.\n\nThere also seems to be a correlation between **number of attempts and number of off-targets and on-targets** but those are similar so we discount that inference.\n\nThere seems to be a relation between **Ball Possession and Pass Accuracy**.\n\nThere is a correlation between **number of attempts and Corners**. This seems logical as a corner kick is the method of restarting play when the ball goes out of play over the goal line, without a goal being scored, and having last been touched by a member of the defending team. The kick is taken from the corner of the field of play nearest to where it went out. So if attempts were made which weren't successful then it would lead to more number of corner kicks.\n\n\n\n"},{"metadata":{"_uuid":"1335dc76ec4f22e4bb38e572744bbfe134a4f43e"},"cell_type":"markdown","source":"### Relationship Between Ball Possession % and Passes"},{"metadata":{"trusted":true,"_uuid":"a61ae823a9bc66aa83cf62a4cef0593704f19cb4"},"cell_type":"code","source":"plt.figure(figsize = (15,15))\nplt.scatter(x=\"Ball Possession %\", y=\"Passes\", data=data_file)\nplt.xlabel(\"Ball Possession %\")\nplt.ylabel(\"Passes\")\nplt.title('Relationship Between Ball Possession % and Passes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0302f7ddb2f357098021e1d75e38cc9dca090dbc"},"cell_type":"markdown","source":"**The above scatter plot shows a very clean linear relationship between Ball Possession % and Passes. Thus, this hypothesis is validated. As the more number of passes made by a team means that the ball is in their possession.**"},{"metadata":{"_uuid":"fb78918e88e5a37e1ecf08102b15e7234bc3526e"},"cell_type":"markdown","source":"### Relationship between Attempts and Corners"},{"metadata":{"trusted":true,"_uuid":"9f23f8f31cc4ddc0f398db6441664163a6382c44"},"cell_type":"code","source":"plt.figure(figsize = (15,15))\nplt.scatter(x=\"Attempts\", y=\"Corners\", data=data_file)\nplt.xlabel(\"Attempts\")\nplt.ylabel(\"Corners\")\nplt.title(\"Relationship between Attempts and Corners\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2588f3187310baea94b2759b5ad9fcb735a0219"},"cell_type":"markdown","source":"**This scatter plot is not very clean this shows high variance but both the features do have a linear relationship.**"},{"metadata":{"_uuid":"77225f56336f71addb4580f4913eca25798ce711"},"cell_type":"markdown","source":"### Fair Play\nPenalty Cards received by players of each team.\nA Yellow Card is received by a player in order to discipline them.\nReceival of two yellow cards in one game leads to the receival of a Red Card, which dismisses the player from the field for the remaining match and may ban them from playing the subsequent match.\n\n#### Total Yellow Cards Received"},{"metadata":{"trusted":true,"_uuid":"554612ea16c230ad2c63b49abf0a4e4e9868a630"},"cell_type":"code","source":"yellow_cards_data = data_file.groupby('Team')['Yellow Card'].sum().reset_index().sort_values(by=('Yellow Card'), ascending=True)\n\nplt.figure(figsize = (16, 10), facecolor = None)\nsns.set_style(\"darkgrid\")\nsns.set(font_scale=1.5)\nplot1 = sns.barplot(x=\"Team\", y=\"Yellow Card\", data=yellow_cards_data)\n\nplot1.set_xticklabels(yellow_cards_data['Team'], rotation=90, ha=\"center\")\nplot1.set(xlabel='Teams',ylabel='Total yellow cards')\nplot1.set_title('Total yellow cards')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b25331fb5a7ee02240fe679e8abb2d45359348e"},"cell_type":"markdown","source":"#### Total Red Cards Received\nAs was seen from the initial data analysis. Only players from two teams have received Red Cards these are Columbia and Switzerland. They both got 1 red card each."},{"metadata":{"_uuid":"14efaa20a876e7dbb89666d9a8bf4eaadd5d0656"},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{"_uuid":"5f4640ee16a59f0cb97e2139e2f5babeda52544a"},"cell_type":"markdown","source":"Total Man of the Match won by each team"},{"metadata":{"trusted":true,"_uuid":"e458417ea3191d959480427f71f70af8fb31bf0d"},"cell_type":"code","source":"mom_data = data_file.groupby('Team')['MoM'].sum().reset_index().sort_values(by=('MoM'),ascending=True)\n\nplt.figure(figsize = (17, 10), facecolor = None)\nsns.set_style(\"darkgrid\")\nsns.set(font_scale=1.5)\nplot1 = sns.barplot(x=\"Team\", y=\"MoM\", data=mom_data)\n\nplot1.set_xticklabels(mom_data['Team'], rotation=90, ha=\"center\")\nplot1.set(xlabel='Teams',ylabel='Total Man of the Matches')\nplot1.set_title('Most Man of the Match awards')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3dfcca56fcd52cc1c8a49cd349340798098e4f9"},"cell_type":"markdown","source":"Here it can be seen that Australia, Saudi Arabia,  Costa Rica, Panama and Morocco have won zero Man of the Match awards. Most were won by France, the team which eventually went onto win the world cup."},{"metadata":{"_uuid":"ca4272fd8a8212764be2616318127e89e6e899ed"},"cell_type":"markdown","source":"### RADAR CHART\nThis chart compares the performace of the two finalists: France and Croatia, throughout the tournament."},{"metadata":{"trusted":true,"_uuid":"180cae98733075416e1be2090652c04eedaf9aa9"},"cell_type":"code","source":"from math import pi\n\ncategories=np.array(['Fouls Committed', 'Ball Possession %', 'Goal Scored', 'Attempts', 'Blocked', 'On-Target', 'Off-Target', 'Corners', 'Blocked', 'Offsides', 'Saves', 'Distance Covered (Kms)', 'MoM'])\nfeatures_cumulative = data_file.groupby('Team')['Fouls Committed', 'Ball Possession %', 'Goal Scored', 'Attempts', 'Blocked', 'On-Target', 'Off-Target', 'Corners', 'Offsides', 'Saves', 'Distance Covered (Kms)', 'MoM'].sum().reset_index()\n\nfeatures_cumulative['Distance Covered (Kms)'] = features_cumulative['Distance Covered (Kms)']/10\ndata_file.groupby('Team').count()\nfeatures_cumulative['Ball Possession %'] = features_cumulative['Ball Possession %']/7\nfeatures_cumulative['MoM']=features_cumulative['MoM']*10\n\nstats=features_cumulative.loc[10,categories].values \nstats=np.concatenate((stats,[stats[0]]))  #Closes the plot\n\n# Setting an angle (we divide the plot / number of variable)\nangles=np.linspace(0, 2*np.pi, len(categories), endpoint=False)   \nangles=np.concatenate((angles,[angles[0]]))\n\n# Initialising the radar plot\nplot1 = plt.figure(figsize=(12, 12))\nplot1 = plt.subplot(111, polar=True,)\n\nplot1.set_theta_offset(pi / 2)\nplot1.set_theta_direction(-1)\n \nplt.xticks(angles[:], categories, size=20)\n \n# Draw ylabels\nplot1.set_rlabel_position(0)\nplt.yticks([10,20,30, 40, 50, 60, 70 , 80 , 90 , 100, 110, 120, 130], [\"10\",\"20\",\"30\",\"40\", \"50\", \"60\",  '70' , '80' , '90' , '100', '110', '120', '130'], color=\"grey\", size=15)\nplt.ylim(0,130)\n \n#France plot\nplot1.plot(angles, stats, linewidth=1, linestyle='solid', label=\"France\")\nplot1.fill(angles, stats, 'b', alpha=0.1)\n \n#Croatia plot\nstats=features_cumulative.loc[6,categories].values \nstats=np.concatenate((stats,[stats[0]]))\nplot1.plot(angles, stats, linewidth=1, linestyle='solid', label=\"Croatia\")\nplot1.fill(angles, stats, 'r', alpha=0.1)  #alpha sets the opacity\n \nplt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), prop={'size': 25})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55c61f9bb08678d0fe5355d4c9b498b5d6a47b2f"},"cell_type":"markdown","source":"** From this radar chart it can be observed that cummulatively Croatia committed more fouls than France.\nCroatia also made more attempts and eventually scored more goals than France.\nIt seems Croatia played in an offensive manner while France played in a defensive manner. **\n\n"},{"metadata":{"_uuid":"1b911aee71a2193a72856062ec275b6bbb669ecb"},"cell_type":"markdown","source":"## Man of the Match Prediction\n"},{"metadata":{},"cell_type":"markdown","source":"#### Encoding the Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_file.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Team = pd.get_dummies(data_file['Team'], prefix = 'Team', drop_first = True)\nOpponent = pd.get_dummies(data_file['Opponent'], prefix = 'Opponent', drop_first = True)\nRound = pd.get_dummies(data_file['Round'], prefix = 'Round', drop_first = True)\nPSO = pd.get_dummies(data_file['PSO'], prefix = 'PSO', drop_first = True)\n\ndata_file.drop(['Team', 'Opponent', 'Round', 'PSO', 'Date'], inplace = True, axis = 1)\ndata_file = pd.concat([data_file,Team,Opponent, Round, PSO],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_file.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3921b8330def6e93a32a639df2ca30c6ce9027bb"},"cell_type":"markdown","source":"### Features Importance\nUsing Permutation Importance\nFor this we follow these steps:\n1. Get a trained model\n2. Shuffle the values in a single column, make predictions using the resulting dataset. Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. That performance deterioration measures the importance of the variable you just shuffled.\n3. Return the data to the original order (undoing the shuffle from step 2 and repeat step 2 with the next column in the dataset, until we have calculated the importance of each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5, random_state=None)\n\n# X is the feature set and y is the target\ny = data_file['MoM']\nX = data_file.drop('MoM', axis=1)\n#feature_names = [i for i in data_NoMoM.columns if data_NoMoM[i].dtype in [np.int64]]\n#X = data_NoMoM[feature_names]\n\nfor train_index, test_index in skf.split(X,y): \n    #print(\"Train:\", train_index, \"Test:\", test_index) \n    X_train, X_test = X.iloc[train_index], X.iloc[test_index] \n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nDTree_Model = DecisionTreeClassifier()\nDTree_Model.fit(X_train, y_train)\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(DTree_Model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad2e3643aea77b85cf8167889d81f6bdefe2af83"},"cell_type":"markdown","source":"ELI5 is a Python package which helps to debug machine learning classifiers and explain their predictions. "},{"metadata":{"_uuid":"e0cec9213958ea7bb4b5ba90c870657c1825eac0"},"cell_type":"markdown","source":"The values towards the top are the most important features, and those towards the bottom matter least.\n\nThe first number in each row shows how much model performance decreased with a random shuffling. The number after the ± measures how performance varied from one-reshuffling to the next.\n\nYou'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. \n\nHere the Goal Scored is the most important feature, and that seems logical.\n"},{"metadata":{"_uuid":"cc3c4294ac48d92399bc8d98e79c4f9c902a0e51"},"cell_type":"markdown","source":"## Using Machine Learning Models to Predict Man of the Match\n\n### Data Preparation\nData has already been split into Training and Test sets using stratified k-fold cross validation."},{"metadata":{"_uuid":"399560549669f20d02302cf23b9ab0ff64a2813f"},"cell_type":"markdown","source":"### Using The Decision Tree Classifier\n\nDecision tree is a type of supervised learning algorithm.\nIt works for both categorical and continuous input and output variables. In this technique, we split the sample into two or more homogeneous sets based on most significant splitter / differentiator in input variables.\n\nA decision tree is drawn upside down with its root at the top. An internal node is actually a condition based on which the tree splits into branches/ edges. The end of the branch that doesn’t split anymore is the decision/leaf."},{"metadata":{"trusted":true,"_uuid":"5544bd68f01c7bb251f1a84bd122f694babd85df"},"cell_type":"code","source":"DTree_Model = DecisionTreeClassifier()\nDTree_Model.fit(X_train, y_train)\n\nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator=DTree_Model, X=X_train, y=y_train, cv=10)\ndt_model_accuracy = accuracies.mean()\ndt_model_standard_deviation = accuracies.std()\nprint(\"model accuracy\", dt_model_accuracy)\nprint(\"model standard deviation\", dt_model_standard_deviation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, DTree_Model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\narea_under_curve = roc_auc_score(y_test, DTree_Model.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, DTree_Model.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Decision Tree (area = %0.2f)' % area_under_curve)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n#plt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall : Proportion of correct positive classification (True positives) from cases that are actually positive. <br>\nRecall = TP/(TP+FN) = True Positive/ Actual Positives<br>\n<br>\nPrecision : Proportion of correct positive classification (True positives) from cases that are predicted as positive. <br>\nPrecision = TP/(TP+FP) = True Positive/ Predicted Positives <br>\n<br>\nF1-Score : The F1 score conveys the balance between the precision and the recall. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).<br>\nF1-Score = 2x((precision x recall)/(precision+recall))\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\npredicted = DTree_Model.predict(X_test)\nprint(metrics.classification_report(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7d130aad774d3dc82021ca9994dcdef7cde4fff"},"cell_type":"markdown","source":"### Using The Random Forest Classifier"},{"metadata":{"_uuid":"2d4c25d407c23445fa9792d0737e0152637b881f"},"cell_type":"markdown","source":"Random Forest is a supervised learning algorithm. It is an ensemble of Decision Trees. It uses the bagging method which is that a combination of learning models increases the overall result.\n"},{"metadata":{"trusted":true,"_uuid":"b640a1cc8c0cc6fea450f43b594f8b32c17a5eff"},"cell_type":"code","source":"RForest_Model = RandomForestClassifier()\nRForest_Model.fit(X_train, y_train)\n\naccuracies = cross_val_score(estimator=RForest_Model, X=X_train, y=y_train, cv=10)\nrf_model_accuracy = accuracies.mean()\nrf_model_standard_deviation = accuracies.std()\nprint(\"model accuracy\", rf_model_accuracy)\nprint(\"model standard deviation\", rf_model_standard_deviation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, RForest_Model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\narea_under_curve = roc_auc_score(y_test, RForest_Model.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, RForest_Model.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' % area_under_curve)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n#plt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\npredicted = RForest_Model.predict(X_test)\nprint(metrics.classification_report(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d39885c4d4ef3f70f7b2a691fcd0b8d10461c2d1"},"cell_type":"markdown","source":"### Using The Gradient Boosting Classifier\n\nThe idea of boosting came out of the idea of whether a weak learner can be modified to become better.\nA weak hypothesis or weak learner is defined as one whose performance is at least slightly better than random chance.\n\nWe go through cycles that repeatedly builds new models and combines them into an ensemble model. We start the cycle by calculating the errors for each observation in the dataset. We then build a new model to predict those. We add predictions from this error-predicting model to the \"ensemble of models.\"\n\nTo make a prediction, we add the predictions from all previous models. We can use these predictions to calculate new errors, build the next model, and add it to the ensemble.\n\n"},{"metadata":{"trusted":true,"_uuid":"25098be3b01d2b4ff641bd0232b95695f8f84e72"},"cell_type":"code","source":"from xgboost import XGBClassifier\nGBoost_Model = XGBClassifier(learning_rate = 0.05, n_estimators= 100, max_depth = 4)\nGBoost_Model.fit(X_train, y_train)\n\naccuracies = cross_val_score(estimator=GBoost_Model, X=X_train, y=y_train, cv=10)\nxgb_model_accuracy = accuracies.mean()\nxgb_model_standard_deviation = accuracies.std()\nprint(\"model accuracy\", xgb_model_accuracy)\nprint(\"model standard deviation\", xgb_model_standard_deviation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, GBoost_Model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\narea_under_curve = roc_auc_score(y_test, GBoost_Model.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, GBoost_Model.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='XGboost (area = %0.2f)' % area_under_curve)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n#plt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\npredicted = GBoost_Model.predict(X_test)\nprint(metrics.classification_report(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using SVC"},{"metadata":{},"cell_type":"markdown","source":"-SVM can be used to do binary classification <br>\n-SVM finds a hyper-plane (line in 2d, plane in 3d, etc) that separates its training data in such a way that the distance between the hyper plane and the closest points from each class is maximized. <br>\n-once SVM finds this hyper-plane, you can classify new data points by seeing which side of this hyper-plane they land on <br>\n-SVM can only be used on data that is linearly separable (i.e. a hyper-plane can be drawn between the two groups)"},{"metadata":{},"cell_type":"markdown","source":"#### y = w<sup>T</sup> + b\ny : classification label\nw : parameters of the plane\nb : moves hyperplane in and out of the origin\n\nHyperplane is also known as the decision boundary, it separates the two classes of data. While also having the property of being the most further away from both classes.\nEquation for the hyperplane is <br>w<sup>T</sup> + b = 0\n![](https://cdn-images-1.medium.com/max/1600/1*TudH6YvvH7-h5ZyF2dJV2w.jpeg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_cls = SVC()\nsvc_cls.fit(X_train_scaled, y_train)\nsvc_accuracy = accuracy_score(y_test, svc_cls.predict(X_test_scaled))\nprint('accuracy score', svc_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('confusion matrix')\nprint(confusion_matrix(y_test, svc_cls.predict(X_test_scaled)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\npredicted = svc_cls.predict(X_test_scaled)\nprint(metrics.classification_report(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eafbb8608c4c164e42895c43df61c8ca431ee5be"},"cell_type":"markdown","source":"### Plotting the Accuracy Percentage of all the Classifiers"},{"metadata":{"trusted":true,"_uuid":"6a3daa547a25c22cce872459b313bfd773c1690d"},"cell_type":"code","source":"plt.figure(figsize = (25,10))\n\narr = np.array((dt_model_accuracy*100))\narr = np.append(arr, (rf_model_accuracy*100))\narr = np.append(arr, (xgb_model_accuracy*100))\narr = np.append(arr, (svc_accuracy*100))\nser = pd.Series(arr)\nprint('Accuracy for Decision Tree', arr[0])\nprint('Accuracy for Random Forest', arr[1])\nprint('Accuracy for Gradient Boost Classifier', arr[2])\nprint('Accuracy for Support Vector Classifier', arr[3])\n\nsns.set(font_scale=2.2)\nsns.set_style(\"darkgrid\")\nModels_labels = ['Decision Trees', 'Random Forest', 'Gradient Boosting Classifier', 'Support Vector Classifier']\nplot1 = sns.barplot(x=Models_labels, y=ser)\n\nplot1.set_xticklabels(Models_labels, ha=\"center\")\nplot1.set(xlabel='Models',ylabel='Accuracy')\nplot1.set_title('Accuracy % of Each Model')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}